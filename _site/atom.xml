<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Deep Learning Experience</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/css/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/css/favicon.ico" mce_href="/favicon.ico" type="image/x-icon">
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>

<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Deep Learning Experience</title>
   <link href="http://localhost:3000/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://localhost:3000" rel="alternate" type="text/html" />
   <updated>2017-12-20T19:02:38+08:00</updated>
   <id>http://localhost:3000</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>cnn迁移学习详解与实践技巧</title>
     <link href="/transfer-learning-practice-technique"/>
     <updated>2016-12-23T00:00:00+08:00</updated>
     <id>/transfer-learning-practice-technique</id>
     <content type="html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;迁移学习是指我们从网络上下载训练好的模型也就是weight，然后部分的嵌入我们的网络，作为初始化weight或者在此基础上用我们自己的数据集进行微调。
&lt;br /&gt;&lt;br /&gt;因为训练神经网络花费时间巨大有时甚至要数周， 因此在实际开发中，我们很少从头开始去训练网络，而在网络上已经有很多为不同任务使用大型数据集训练了数周的模型，这些模型均已经稳定收敛，达到了很高的精度。&lt;br /&gt;&lt;br /&gt;因此除非你的需求非常独特，否则大部分情况下你都可以使用迁移学习,在这些模型的基础上用自己的数据集进行训练，或者把这些模型部分的嵌入自己的网络，比如在分割或检测任务中，如果是用滑动窗口方法，那对于该窗口而言，仍然是一个分类任务，可以迁移你想要的分类模型，当然现在的分割和检测已经不直接使用滑动窗口，但是只要你网络中的部分任务是前人做过，你都可以使用迁移学习的方法把weight嵌入你的网络作为初始化或者微调。&lt;/p&gt;

&lt;h2 id=&quot;1-微调技巧&quot;&gt;1. 微调技巧&lt;/h2&gt;
&lt;p&gt;在开发中针对不同的情况，对模型的微调要采用不同的方法,否则反而会得出较差的结果或者要花费更长的时间训练.大概情况有这么几种:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;数据集很相似&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;数据集不相似&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;数据很少(10000左右)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;只训练分类层&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;尝试从不同层训练分类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;数据不算少也不多&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;微调最后几层&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;微调最后多层&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;大量数据，几百万&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;把模型参数当初始化训练&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;把模型参数当初始化训练&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;如上表所示，对于我们能拥有的数据集大小和任务相似性，要分不同情况处理：&lt;br /&gt;
&lt;img src=&quot;../images/cnn_transfer_learning_freeze_layers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;1-similar-and-small-dataset&quot;&gt;1. similar and small dataset&lt;/h3&gt;
&lt;p&gt;数据集很相似数据很少，比如我们下载的是在ImageNet上训练好的inceptionV4 1000分类模型，而我们想做的是对20种猫进行分类，我们可以直接使用该模型不包含分类层的所有权重，并在此基础上训练20种分类，如上图2.SmallDataSet所示.&lt;br /&gt;有一点要注意的是，对于冻结层的处理，有两种方法:&lt;/p&gt;
&lt;h4 id=&quot;a-每次训练让image从头输入网络但是不训练冻结层的权重&quot;&gt;a. 每次训练让image从头输入网络，但是不训练冻结层的权重.&lt;/h4&gt;
&lt;h4 id=&quot;b让image输入网络直到冻结层结束也就是图片最后一层fc-4096输出feature这里是4096维的feature存储在本地然后训练的时候把feature当作输入后面接我们的分类层这样做可以大大提高训练速度所以一般采用该方法后面的代码演示也是使用该方法&quot;&gt;b.让image输入网络，直到冻结层结束，也就是图片最后一层FC-4096,输出feature这里是4096维的feature，存储在本地。然后训练的时候把feature当作输入,后面接我们的分类层。这样做可以大大提高训练速度，所以一般采用该方法，后面的代码演示也是使用该方法。&lt;/h4&gt;
&lt;h3 id=&quot;2-similar-and-many-dataset&quot;&gt;2. similar and many dataset&lt;/h3&gt;
&lt;p&gt;数据集很相似数据不算少也不多,这里的不多是指相对于我们下载的模型的训练集而言，比如我们拥有数十万的训练集，此时可以对网络最后几层进行训练，具体几层取决于我们拥有的数据集。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;也有一点要注意的是，进行微调的时候最好把最后层的学习率设为此模型正常训练时学习率的1/10，中间微调层的学习率设为1/100，或者先微调最后分类层，等模型收敛后，在把最后几层加入微调.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;3-similar-and-a-lot-of-dataset&quot;&gt;3. similar and a lot of dataset&lt;/h3&gt;
&lt;p&gt;数据集很相似同时拥有大量数据，此时可以直接把模型参数当做权重初始化对所有参数进行训练，当然也要注意学习率要设为之前的1/10或1/100或更低.&lt;/p&gt;
&lt;h3 id=&quot;4-diffrenet-dataset&quot;&gt;4. diffrenet dataset&lt;/h3&gt;
&lt;p&gt;对数据集不相似的情况，在数据很少时训练效果不会特别好，但是依然可以按照表中的方法进行微调尝试.&lt;/p&gt;
&lt;h3 id=&quot;5代码&quot;&gt;5.代码&lt;/h3&gt;
&lt;p&gt;最后贴一个image直接输入网络，但只对模型部分参数进行训练的代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;optimizer = tf.train.AdagradOptimzer(0.01)
#获取你需要训练的域下的变量
first_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                 &quot;scope/prefix/for/first/vars&quot;)
#最小化cost，只训练first_train_vars
first_train_op = optimizer.minimize(cost, var_list=first_train_vars)

second_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                  &quot;scope/prefix/for/second/vars&quot;)                     
second_train_op = optimizer.minimize(cost, var_list=second_train_vars)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2迁移学习代码实现&quot;&gt;2.迁移学习代码实现&lt;/h2&gt;
&lt;p&gt;演示使用VGG16分类模型，你也可以下载inceptionV3 V4模型取得更好的结果，流程都是一样的&lt;/p&gt;

&lt;h3 id=&quot;1下载vgg16数据&quot;&gt;1.下载VGG16数据&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from urllib.request import urlretrieve
from os.path import isfile,isdir
from tqdm import tqdm

#设置本地存储目录
vgg_dir = 'tensorflow_vgg/'
if not isdir(vgg_dir):
	raise Exception(&quot;VGG directory doesn't exist!&quot;)

#使用tqdm显示下载进度
class DLProgress(tqdm):
	last_block = 0
	def hook(self,block_num =1,block_size=1,total_size=None):
    	self.total = total_size
    	self.update((block_num - self.last_block) * block_size)
    	self.last_block = block_num
#如果文件不存在就下载，如果存在就跳过
if not isfile(vgg_dir + &quot;vgg16.npy&quot;):
	#使用urlretrieve下载模型数据,vgg全部参数大概500多M,所以也可以手动下载，然后放到tensorflow_vgg文件夹里
	with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:
    	urlretrieve('https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',vgg_dir + 'vgg16.npy',pbar.hook)
else :
   		print(&quot;Parameter file already exists!&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-下载flower数据&quot;&gt;2. 下载flower数据&lt;/h3&gt;
&lt;p&gt;演示使用google的flower数据，与上面下载逻辑基本一致&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#tarfile解压缩
import tarfile

dataset_folder_path = 'flower_photos'

class DLProgress(tqdm):
	last_block = 0

	def hook(self, block_num=1, block_size=1, total_size=None):
    	self.total = total_size
    	self.update((block_num - self.last_block) * block_size)
    	self.last_block = block_num

if not isfile('flower_photos.tar.gz'):
	with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:
    	urlretrieve(
        	'http://download.tensorflow.org/example_images/flower_photos.tgz',
        	'flower_photos.tar.gz',
        	pbar.hook)
#解压缩
if not isdir(dataset_folder_path):
	with tarfile.open('flower_photos.tar.gz') as tar:
    	tar.extractall()
    	tar.close()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-图片通过冻结层输出feature&quot;&gt;3. 图片通过冻结层，输出feature&lt;/h3&gt;

&lt;h4 id=&quot;a-导如需要包&quot;&gt;a. 导如需要包&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import os
import numpy as np
import tensorflow as tf
from tensorflow_vgg import vgg16
from tensorflow_vgg import utils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;b-获取图片分类&quot;&gt;b. 获取图片分类&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#获取图片路径
data_dir = 'flower_photos/'
#获取图片分类
contents = os.listdir(data_dir)
print('contents contain {}'.format(contents))
#再次获取图片分类，去掉不是文件夹的路径
classes = [each for each in contents if isdir(data_dir+each)]
print('classes = {}'.format(classes))
#classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;c-处理图片获取分类层之前的feature&quot;&gt;c. 处理图片,获取分类层之前的feature&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;batch_size = 10
codes_list = []
labels = []
batch = []
codes = None

#重置计算图,防止重复加载
tf.reset_default_graph()
with tf.Session() as sess:
    feed_images = tf.placeholder(dtype=tf.float32,shape=[None,224,224,3],name='feed_image')
    
    #加载vgg16模型,bulid运算图
    vgg = vgg16.Vgg16()
    with tf.name_scope('content_vgg'):
        vgg.build(feed_images)
        
    #递归分类
    for each in classes :
        print(&quot;Starting {} images&quot;.format(each))
        class_path = data_dir + each
        files = os.listdir(class_path)
        
        #递归一个分类中的所有图片
        for ii ,file in enumerate(files,1):
            #加载图片，中心裁剪，并shape成224x224x3
            img = utils.load_image(os.path.join(class_path,file))
            #把图片和对应分类组成列表
            batch.append(img.reshape((1, 224, 224, 3)))
            labels.append(each)
            
            if ii % batch_size == 0 or ii == len(files):
                images = np.concatenate(batch)
                
                #输入是images，输出是运行到relu6层的结果
                codes_batch = sess.run(vgg.relu6,feed_dict={feed_images:images})
                
                #把结果拼接为codes
                if codes is None :
                    codes = codes_batch
                else:
                    codes = np.concatenate((codes,codes_batch))
                    
                #重置batch
                batch = []
                print('{} images processed'.format(ii))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;d-保存features&quot;&gt;d. 保存features&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# write codes to file
with open('codes','w') as f:
    codes.tofile(f)

# write labels to file
import csv
with open('labels','w') as f:
    writer = csv.writer(f,delimiter='\n')
    writer.writerow(labels)	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行结果是：codes是一个[3670,4096]数组,3670表示图片数量，4096是特征维度，labels是[3670,5],5表示总共分5类&lt;/p&gt;

&lt;h3 id=&quot;4-建立分类器进行训练&quot;&gt;4 建立分类器进行训练&lt;/h3&gt;
&lt;h4 id=&quot;a加载codes和lables上面图片只处理一次以后都直接从硬盘加载&quot;&gt;a.加载codes和lables，上面图片只处理一次，以后都直接从硬盘加载&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# read codes and labels from file
import csv
with open('labels') as f:
    reader = csv.reader(f,delimiter='\n')
    labels = np.array([each for each in reader if len(each) &amp;gt; 0]).squeeze()

with open('codes') as f:
    codes = np.fromfile(f,dtype=np.float32)
    codes = codes.reshape((len(labels),-1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;b.对lables进行onehot编码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#use LabelBinarizer to create one-hot encoded vectors from the labels.
from sklearn.preprocessing import LabelBinarizer

lb = LabelBinarizer()
lb.fit(labels)
labels_vecs = lb.transform(labels) c.随机打乱数据集次序，并分成train val test集合:

#shuffle our data so the validation and test sets contain data from all classes

from sklearn.model_selection import StratifiedShuffleSplit
ss = StratifiedShuffleSplit(n_splits=1,test_size=0.2)
train_idx, val_idx = next(ss.split(codes,labels))

half_val_len = int(len(val_idx)/2)
val_idx ,test_idx = val_idx[:half_val_len],val_idx[half_val_len:]

train_x, train_y = codes[train_idx],labels_vecs[train_idx]
val_x, val_y = codes[val_idx],labels_vecs[val_idx]
test_x, test_y = codes[test_idx],labels_vecs[test_idx]

#Train shapes (x, y): (2936, 4096) (2936, 5)
#Validation shapes (x, y): (367, 4096) (367, 5)
#Test shapes (x, y): (367, 4096) (367, 5)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;d.构建分类层，logits，损失函数，预测准确率:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#build the classifier layers

#placeholder input and labels 
inputs_ = tf.placeholder(dtype=tf.float32,shape=[None, codes.shape[1]],name='inputs_place')
labels_ = tf.placeholder(dtype=tf.float32,shape=[None,labels_vecs.shape[1]],name='lables_place')

# fully_connected  and cost
fc = tf.contrib.layers.fully_connected(inputs_,256)
logits = tf.contrib.layers.fully_connected(fc,num_outputs=labels_vecs.shape[1],activation_fn=None)
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_,logits=logits)
cost = tf.reduce_mean(cross_entropy)

#adam optimizer to minimize cost
optimizer = tf.train.AdamOptimizer().minimize(cost)

#预测准确率
predicted = tf.nn.softmax(logits)
correct_pred = tf.equal(tf.arg_max(predicted,1),tf.arg_max(labels_,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;e.把训练集分成批量进行训练:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#get batch 
def get_batches(x, y, n_batches=10):
    batch_size = len(x) // n_batches
    
    for ii in range(0,n_batches * batch_size ,batch_size):
        if ii != (n_batches-1) * batch_size:
            X,Y = x[ii: ii+batch_size],y[ii: ii+batch_size]
        else :
            X,Y = x[ii:],y[ii:]
    
        yield X,Y f.训练并保存模型:

epochs = 10
iteration = 0
saver = tf.train.Saver()
with tf.Session() as sess:
    
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        for x ,y in get_batches(train_x,train_y) :
            
        
            loss, _ = sess.run([cost,optimizer],feed_dict={inputs_:x,labels_ : y})
            print(&quot;Epoch: {}/{}&quot;.format(e+1, epochs),
                      &quot;Iteration: {}&quot;.format(iteration),
                      &quot;Training loss: {:.5f}&quot;.format(loss))
            iteration += 1
        
            if iteration % 5 ==0:
                feed = {inputs_: val_x,labels_: val_y}
                val_acc = sess.run(accuracy,feed_dict=feed)
                print(&quot;Epoch: {}/{}&quot;.format(e, epochs),
                          &quot;Iteration: {}&quot;.format(iteration),
                          &quot;Validation Acc: {:.4f}&quot;.format(val_acc))
    saver.save(sess,&quot;checkpoints/flowers.ckpt&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;结果：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Epoch: 9/10 Iteration: 85 Training loss: 0.01775
Epoch: 9/10 Iteration: 86 Training loss: 0.03539
Epoch: 9/10 Iteration: 87 Training loss: 0.02733
Epoch: 9/10 Iteration: 88 Training loss: 0.01915
Epoch: 9/10 Iteration: 89 Training loss: 0.02459
Epoch: 8/10 Iteration: 90 Validation Acc: 0.8610
Epoch: 10/10 Iteration: 90 Training loss: 0.01595
Epoch: 10/10 Iteration: 91 Training loss: 0.02532
Epoch: 10/10 Iteration: 92 Training loss: 0.01329
Epoch: 10/10 Iteration: 93 Training loss: 0.02923
Epoch: 10/10 Iteration: 94 Training loss: 0.02558
Epoch: 9/10 Iteration: 95 Validation Acc: 0.8638
Epoch: 10/10 Iteration: 95 Training loss: 0.01392
Epoch: 10/10 Iteration: 96 Training loss: 0.02822
Epoch: 10/10 Iteration: 97 Training loss: 0.02173
Epoch: 10/10 Iteration: 98 Training loss: 0.01468
Epoch: 10/10 Iteration: 99 Training loss: 0.01980
Epoch: 9/10 Iteration: 100 Validation Acc: 0.8610
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这是只训练了10次的结果，loss并没有降到很低，如果继续训练，并使用数据增强，应该能取得更好效果，从这里可以看出使用迁移学习可以很轻松的获得一个相应的高准确率的结果,这种方法还是强大的.&lt;/p&gt;

&lt;h3 id=&quot;5-测试&quot;&gt;5. 测试&lt;/h3&gt;
&lt;h4 id=&quot;a使用测试集测试&quot;&gt;a.使用测试集测试:&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#Testing
with tf.Session() as sess:
    saver.restore(sess,tf.train.latest_checkpoint('checkpoints'))
    feed = {inputs_: test_x,
            labels_: test_y}
    test_acc = sess.run(accuracy, feed_dict=feed)
    print(&quot;Test accuracy: {:.4f}&quot;.format(test_acc))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行结果:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test accuracy: 0.8719
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;b使用训练的新模型对一朵花进行实际预测&quot;&gt;b.使用训练的新模型，对一朵花进行实际预测:&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%matplotlib inline

import matplotlib.pyplot as plt
from scipy.ndimage import imread

test_img_path = 'flower_photos/roses/10894627425_ec76bbc757_n.jpg'
test_img = imread(test_img_path)
plt.imshow(test_img)

# Run this cell if you don't have a vgg graph built
with tf.Session() as sess:
    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])
    vgg = vgg16.Vgg16()
    vgg.build(input_)

with tf.Session() as sess:
    img = utils.load_image(test_img_path)
    img = img.reshape((1, 224, 224, 3))

    feed_dict = {input_: img}
    code = sess.run(vgg.relu6, feed_dict=feed_dict)
        
saver = tf.train.Saver()
with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))
    
    feed = {inputs_: code}
    prediction = sess.run(predicted, feed_dict=feed).squeeze()

plt.imshow(test_img)

plt.barh(np.arange(5), prediction)
_ = plt.yticks(np.arange(5), lb.classes_)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行结果：
&lt;img src=&quot;../images/cnn_transfer_learning_flower1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../images/cnn_transfer_learning_flower1_predict_result.png&quot; alt=&quot;&quot; /&gt;
很棒！我们成功的预测了图片里花为玫瑰.&lt;/p&gt;

&lt;h2 id=&quot;3-总结&quot;&gt;3. 总结&lt;/h2&gt;
&lt;p&gt;使用迁移学习可以很容易的得出我们想要的模型，但是里面的坑还是很多的，最重要的还是要记住，微调时学习率一定要比正常的小，因为你是在一个已经收敛的模型上进行训练的.&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>cnn可视化与理解(2)—cnn网络认为世界是什么样子的</title>
     <link href="/cnn-work-principle-code-implement"/>
     <updated>2016-11-21T00:00:00+08:00</updated>
     <id>/cnn-work-principle-code-implement</id>
     <content type="html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;在上一篇&lt;a href=&quot;https://andrewhuman.github.io/cnn-hidden-layout_search&quot;&gt;cnn卷积网络每一层是怎么工作的&lt;/a&gt;中我们看了cnn每一层在找什么，除此之外还有几种方法可以帮助我们更好的理解网络，包括网络对图像的哪部分更敏感,你训练出来的网络所认为的人或者其他事物是什么样子的等。&lt;br /&gt;本文是在cs321n练习NetworkVisualization基础上的讲解,更多内容可以查看&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf&quot;&gt;CS231N&lt;/a&gt;和git上的代码实现&lt;/p&gt;
&lt;h2 id=&quot;网络模型&quot;&gt;网络模型&lt;/h2&gt;
&lt;p&gt;网络模型使用&lt;a href=&quot;https://arxiv.org/pdf/1602.07360.pdf&quot;&gt;SqueezeNet&lt;/a&gt;,作者是UC Berkeley等人,它的架构如下图:
&lt;img src=&quot;../images/squeestnet_architect.png&quot; alt=&quot;squeezenet&quot; /&gt;
该模型只有AlexNet五十分之一的参数，却达到了同样的精度，最核心的改变是FireModule:
&lt;img src=&quot;../images/squeestnet_fire_module.png&quot; alt=&quot;&quot; /&gt;
其实就是把一层的卷积变成2层,第一层是1x1的卷积S11，之后是1x1和3x3卷积，记为e11和e33，最后把e11和e33拼接起来，python代码如下:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def fire_module(x,inp,sp,e11p,e33p):
    with tf.variable_scope(&quot;fire&quot;):
        with tf.variable_scope(&quot;squeeze&quot;):
            W = tf.get_variable(&quot;weights&quot;,shape=[1,1,inp,sp])
            b = tf.get_variable(&quot;bias&quot;,shape=[sp])
            s = tf.nn.conv2d(x,W,[1,1,1,1],&quot;VALID&quot;)+b
            s = tf.nn.relu(s)
        with tf.variable_scope(&quot;e11&quot;):
            W = tf.get_variable(&quot;weights&quot;,shape=[1,1,sp,e11p])
            b = tf.get_variable(&quot;bias&quot;,shape=[e11p])
            e11 = tf.nn.conv2d(s,W,[1,1,1,1],&quot;VALID&quot;)+b
            e11 = tf.nn.relu(e11)
        with tf.variable_scope(&quot;e33&quot;):
            W = tf.get_variable(&quot;weights&quot;,shape=[3,3,sp,e33p])
            b = tf.get_variable(&quot;bias&quot;,shape=[e33p])
            e33 = tf.nn.conv2d(s,W,[1,1,1,1],&quot;SAME&quot;)+b
            e33 = tf.nn.relu(e33)
        return tf.concat([e11,e33],3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;具体的其他细节可以直接点击查看论文.&lt;/p&gt;
&lt;h2 id=&quot;1-准备工作&quot;&gt;1. 准备工作&lt;/h2&gt;
&lt;h3 id=&quot;11-首先导入需要的lib&quot;&gt;1.1 首先导入需要的lib&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from __future__ import print_function
import time, os, json
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from cs231n.classifiers.squeezenet import SqueezeNet
from cs231n.data_utils import load_tiny_imagenet
from cs231n.image_utils import preprocess_image, deprocess_image
from cs231n.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD

%matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

def get_session():
	&quot;&quot;&quot;Create a session that dynamically allocates memory.&quot;&quot;&quot;
	config = tf.ConfigProto()
	config.gpu_options.allow_growth = True
	session = tf.Session(config=config)
	return session

%load_ext autoreload
%autoreload 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-下载训练好的squeezenet模型数据也就是weight导入模型&quot;&gt;1.2 下载训练好的squeezenet模型数据也就是&lt;a href=&quot;http://cs231n.stanford.edu/squeezenet_tf.zip&quot;&gt;weight&lt;/a&gt;,导入模型&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_default_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SAVE_PATH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'cs231n/datasets/squeezenet.ckpt'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SqueezeNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SAVE_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;13-导入图片&quot;&gt;1.3 导入图片&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from cs231n.data_utils import load_imagenet_val
X_raw, y, class_names = load_imagenet_val(num=5)

plt.figure(figsize=(12, 6))
for i in range(5):
	plt.subplot(1, 5, i + 1)
	plt.imshow(X_raw[i])
	plt.title(class_names[y[i]])
	plt.axis('off')
plt.gcf().tight_layout()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../images/cnn_newwork_visualization_load_image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;14-对图片做预处理图片像素值减去均值再除以方差&quot;&gt;1.4 对图片做预处理,图片像素值减去均值再除以方差&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X = np.array([preprocess_image(img) for img in X_raw])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-saliency-maps&quot;&gt;2. &lt;a href=&quot;https://arxiv.org/pdf/1312.6034.pdf&quot;&gt;Saliency Maps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我们想知道图像的哪部分对分类任务的影响更大，更准确说是哪些像素对最后的score得分影响更大,方法是计算正确的得分相对于图像每个像素的梯度:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def compute_saliency_maps(X, y, model):
	saliency = None
	#计算输入图像的正确分类的得分
	correct_scores = tf.gather_nd(model.classifier,
                              tf.stack((tf.range(X.shape[0]), model.labels), axis=1))
	#计算得分相对于图像每个像素的梯度值
	grads = tf.gradients(correct_scores,model.image)
	grads = grads[0]
	#对梯度值取绝对值，便于观察
	grads = tf.abs(grads)
	#grads的shape=[H,W,3]，只取3个中的最大值，便于观察
	grads = tf.reduce_max(grads,axis=3)
	saliency = sess.run(grads,feed_dict={model.labels:y,model.image:X})
return saliency
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行查看结果:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def show_saliency_maps(X, y, mask):
mask = np.asarray(mask)
Xm = X[mask]
ym = y[mask]

saliency = compute_saliency_maps(Xm, ym, model)

for i in range(mask.size):
    plt.subplot(2, mask.size, i + 1)
    plt.imshow(deprocess_image(Xm[i]))
    plt.axis('off')
    plt.title(class_names[ym[i]])
    plt.subplot(2, mask.size, mask.size + i + 1)
    plt.title(mask[i])
    plt.imshow(saliency[i], cmap=plt.cm.hot)
    plt.axis('off')
    plt.gcf().set_size_inches(10, 4)
plt.show()

mask = np.arange(5)
show_saliency_maps(X, y, mask)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;结果如下:
&lt;img src=&quot;../images/cnn_salicen_image_map.png&quot; alt=&quot;&quot; /&gt;
颜色越亮表示对最终结果影响越大,可以看出鹌鹑的头部分，狗的头部和毛发，都对分类的最终影响很大。这恰恰也说明网络主要是使用这些部分来区分类别，而对于背景部分，网络并不关心，这与人类区分类别的原理很相似.&lt;/p&gt;

&lt;h2 id=&quot;3-class-visualization卷积网络认为的物体是什么样的&quot;&gt;3. &lt;a href=&quot;https://arxiv.org/pdf/1312.6034.pdf&quot;&gt;Class visualization&lt;/a&gt;卷积网络认为的物体是什么样的&lt;/h2&gt;
&lt;p&gt;我们知道输入一个图片后，在最后一层会给出分值，网络以此对图片进行分类。如果我们有一个图片，可以把某个分类的得分达到最高，就能观察到网络所认为的这个分类应该是什么样的,这是很有趣的尝试,而且能得出漂亮的结果.方法就是我们先随机初始化一个图片，然后与之前相反这次使用梯度增加的方法，让该图片最终可以最大化我们指定的一个分类,最终生成的图片就是我们要的结果，形式化公式如下:
&lt;img src=&quot;../images/cnn_class_visible.png&quot; alt=&quot;&quot; /&gt;
接下来我们看代码实现:&lt;/p&gt;
&lt;h3 id=&quot;31-定义模糊函数使用高斯滤波随机初始化图片&quot;&gt;3.1 定义模糊函数,使用高斯滤波随机初始化图片:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scipy.ndimage.filters import gaussian_filter1d
def blur_image(X, sigma=1):
	X = gaussian_filter1d(X, sigma, axis=1)
	X = gaussian_filter1d(X, sigma, axis=2)
	return X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;32-生成最大化指定分类分值的图片&quot;&gt;3.2 生成最大化指定分类分值的图片:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def create_class_visualization(target_y, model, **kwargs):
	#从kwargs参数中取出我们需要的值
	l2_reg = kwargs.pop('l2_reg', 1e-3) #l2正则化数值，默认1e-3
	learning_rate = kwargs.pop('learning_rate', 25) #学习率 默认25
	num_iterations = kwargs.pop('num_iterations', 200)#迭代次数，默认200
	blur_every = kwargs.pop('blur_every', 10) #每10迭代后使用高斯模糊
	max_jitter = kwargs.pop('max_jitter', 16) #最大抖动范围
	show_every = kwargs.pop('show_every', 25) #每25次迭代后显示下图片
	
	#随机初始化图片
	X = 255 * np.random.rand(224, 224, 3)
	X = preprocess_image(X)[None]

	#取出我们指定的正确分类
	target_ph = model.classifier[:,target_y]
	#正则化
	regulation = l2_reg *tf.pow(tf.norm(model.image) ,2)
	#计算损失,使用损失计算相应的梯度
	loss = target_ph - regulation # scalar loss
	grad = tf.gradients(loss,model.image)[0] # gradient of loss with respect to model.image, same size as model.image
	#循环迭代
  	for t in range(num_iterations):
		#随机抖动图片，这样会让图片更平滑，结果更漂亮
    	ox, oy = np.random.randint(-max_jitter, max_jitter+1, 2)
    	Xi = X.copy()
    	X = np.roll(np.roll(X, ox, 1), oy, 2)
    
    	#计算梯度，并使用梯度增加
    	dgrad = sess.run(grad,feed_dict={model.image:X})
   		X += dgrad
    	#反向抖动图片
    	X = np.roll(np.roll(X, -ox, 1), -oy, 2)
		
		#裁剪，并对图片做高斯模糊
    	X = np.clip(X, -SQUEEZENET_MEAN/SQUEEZENET_STD, (1.0 - SQUEEZENET_MEAN)/SQUEEZENET_STD)
    	if t % blur_every == 0:
        	X = blur_image(X, sigma=0.5)
		#显示图片效果
    	if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:
        	plt.imshow(deprocess_image(X[0]))
        	class_name = class_names[target_y]
        	plt.title('%s\nIteration %d / %d' % (class_name, t + 1, num_iterations))
        	plt.gcf().set_size_inches(4, 4)
        	plt.axis('off')
        	plt.show()
	return X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;33-我们随便选择一个类别运行查看结果&quot;&gt;3.3 我们随便选择一个类别，运行查看结果:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;target_y = np.random.randint(1000)
X = create_class_visualization(target_y, model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../images/cnn_class_visible_result1.png&quot; alt=&quot;&quot; /&gt;
这是神经网络所认为的屋顶,可以看出屋顶的大概结构，但是跟现实的屋顶很显然不太一样，可以认为这是各种屋顶的概念的总和.
&lt;img src=&quot;../images/cnn_class_visible_result_tarantula.png&quot; alt=&quot;&quot; /&gt;
狼蛛，仍然是所有trainset中狼蛛图片的概念总和.&lt;/p&gt;

&lt;h2 id=&quot;4-总结&quot;&gt;4. 总结&lt;/h2&gt;
&lt;p&gt;还有一些方法，例如对输入图片进行部分遮挡以观察最后一层得分变化，fooling image，feature inversion等，都可以让我们进一步理解我们训练好的神经网络到底在做什么，具有什么性质；而且利用这些方法可以生成有趣的图片，比如deepDream,style transfer等。&lt;br /&gt;更重要的是这些方法告诉我们神经网络不是不可解释，不是神秘的,虽然现在还没有严格的数学证明，但是它的理论基础是完善的，或者说可控的.&lt;br /&gt;除此之外，理解这些中间层，对我们设计网络架构是非常有帮助的.&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>cnn可视化与理解(1)—cnn每一层在找什么</title>
     <link href="/cnn-hidden-layout_search"/>
     <updated>2016-11-01T00:00:00+08:00</updated>
     <id>/cnn-hidden-layout_search</id>
     <content type="html">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;虽然神经网络近年来取得了巨大的成功,但神经网络的中间层却一直被视为”黑盒”一样的存在,这造成了外界对神经网络的各种误解,所以近两年学界对中间层的探索做了大量的工作。对于cnn卷积网络而言,从2013年的&lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Visualizing and Understanding Convolutional Networks&lt;/a&gt;开始，人们对卷积网络的可视化和理解进行了很多实验，开发了很多工具。而作为机器学习工程师，只有理解了中间层的作用才能更好的进行网络架构的设计&lt;/p&gt;
&lt;h2 id=&quot;1-cnn卷积层可视化&quot;&gt;1. CNN卷积层可视化&lt;/h2&gt;
&lt;p&gt;本部分内容参考Zeiler 和 Fergus的&lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;论文&lt;/a&gt;和&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf&quot;&gt;CS231N&lt;/a&gt;内容&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/cnn_first_layout_alexnet_2014.png&quot; alt=&quot;AlexNet&quot; /&gt;
上图是ALexNet的第一层原生weight(96x55x55)的直接可视化，每一个单元格都代表了一个卷积核(55x55)，可以从上图中看出每一个卷积核都在寻找一个小的简单结构,包括不同角度的线，不同大小的圆和不同的色块等等.我们也可以直接把第二三四等层的卷积核如上图一样直接可视化,但是这些高层的原生weight或者filter是难以理解和解释的.&lt;/p&gt;

&lt;p&gt;既然原生的filter难以理解，我们必须绕道去观察,其中Deconv approaches让不同的图片通过一个训练好的模型的卷积层，观察其变化,以此探索这些卷积层在寻找什么.它的原理如下：
&lt;img src=&quot;../images/cnn_deconv_approach.png&quot; alt=&quot;&quot; /&gt;
我们输入一张图片，让它通过网络直到我们想观察的那一层，然后把该层我们想观察的神经元的梯度置为1，其他神经元的梯度置为0，从该层开始反向传播，但是在反向传播时要做出一点改变：&lt;/p&gt;
&lt;h4 id=&quot;1-backword-deconvnet方法把反向传播中的所有梯度小于0的全置为0如图红框中所示&quot;&gt;1 backword deconvnet方法把反向传播中的所有梯度小于0的，全置为0，如图红框中所示&lt;/h4&gt;
&lt;h4 id=&quot;2-guided-backpropagatin方法除了把反向传播中梯度小于0的置为0外还把前向传播中小于0的单元也置为0事实上这种方法产生的效果要更好一点&quot;&gt;2 guided backpropagatin方法除了把反向传播中梯度小于0的置为0外，还把前向传播中小于0的单元也置为0,事实上这种方法产生的效果要更好一点&lt;/h4&gt;
&lt;p&gt;这样做的原因是反向传播中负的部分会对我们想得到的图片产生消极影响，从而使图片难以理解，只保留正的部分可以使图片容易观察.&lt;/p&gt;

&lt;p&gt;下面的图片采用的是backword deconvnet方法得到的，可以看到也可以取得不错的效果.&lt;/p&gt;
&lt;h3 id=&quot;11-layer1&quot;&gt;1.1 layer1&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/cnn_first_layout_zeiler.png&quot; alt=&quot;&quot; /&gt;
上图的每个格子同样代表一个卷积核,对于第一行第一列的格子可以粗略看出一条-45度直线,事实上这个格子的卷积核寻找的正是-45度左右的线条，它会对如下图片产生激活反应:
&lt;img src=&quot;../images/cnn_first_layout_0_0_filter_activation.png&quot; alt=&quot;&quot; /&gt;
对于第三行第三列的格子而言，它寻找的是类似下图的色块
&lt;img src=&quot;../images/cnn_first_layout_activation_3_3.png&quot; alt=&quot;&quot; /&gt;
所以总体来说对于一个训练好的模型来说，它的第一层总是在寻找这些简单的结构,不管是AleNet,ResNet还是DenseNet&lt;/p&gt;
&lt;h3 id=&quot;12-layer2&quot;&gt;1.2 layer2&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/cnn_second_layout_filter_activation.png&quot; alt=&quot;&quot; /&gt;
可以看到输入相应图片后，网络激活输出了稍复杂的纹理结构,比如条纹(第一行),嵌套的圆环(第二行右面),色块等等。&lt;/p&gt;
&lt;h3 id=&quot;13-layer3&quot;&gt;1.3 layer3&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/cnn_third_layout_filter_activation.png&quot; alt=&quot;&quot; /&gt;
第三层输出了第二层的组合，比如蜂巢，人,门窗和文字的轮廓&lt;/p&gt;
&lt;h3 id=&quot;14-layer4&quot;&gt;1.4 layer4&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/cnn_fourth_layout_filter_activation.png&quot; alt=&quot;&quot; /&gt;
第四层，我们开始得到一些真实物品形状的东西，例如狗,准确说是狗的抽象形状&lt;/p&gt;

&lt;h3 id=&quot;15-layer5&quot;&gt;1.5 layer5&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/cnn_fifth_layout_filter_activation.png&quot; alt=&quot;&quot; /&gt;
第五层，我们得到一些更高层次的抽象，比如右边第8行4列穿红衣服的女人，输出的是人脸部分，因为对于分类来说，神经网络使用人脸来区分图像是不是表示一个人,所以它只关心人脸部分&lt;/p&gt;

&lt;p&gt;有一个类似的交互性的工作，&lt;a href=&quot;https://www.youtube.com/watch?v=AgkfIQ4IGaM&quot;&gt;视频&lt;/a&gt;和&lt;a href=&quot;http://yosinski.com/deepvis&quot;&gt;网址&lt;/a&gt;,可以实时的看到网络每一层对输入的反应.&lt;/p&gt;
&lt;h2 id=&quot;2-last-layer&quot;&gt;2. last layer&lt;/h2&gt;
&lt;p&gt;对于最后一层，也就是分类层也无法直接对weight进行观察的，不过可以通过降维的方法比如PCA或t-SNE，把它映射到2维平面上,如下图所示:
&lt;img src=&quot;../images/cnn_final_layout_pca.png&quot; alt=&quot;&quot; /&gt;
该图片是对0-9数字的分类任务的最后一层的映射，我们把大量的图片输入网络，在最后一层得到28*28的高维特征，然后降维到2维平面.可以很明显的看到同类的特征会自动聚集到一起，而类与类之间会分离,也就是分类任务已经完成&lt;/p&gt;

&lt;h2 id=&quot;3-总结&quot;&gt;3. 总结&lt;/h2&gt;
&lt;p&gt;这些激活是CNN自己学会的，或者说当我们定义相应loss函数,并用反向传播倒逼其收敛时，cnn每一层的filter会不断调整自己，直到它具有上述能力。&lt;br /&gt;从这里我们也可以想象出过拟合的现象，那就是若过拟合则每层的激活对训练集来说更精准，但适应性也更差.&lt;br /&gt;这是对网络单层的探索，为了解开黑盒，还有很多其他的方法，在&lt;a href=&quot;https://andrewhuman.github.io/cnn-work-principle-code-implement&quot;&gt;下一篇文章&lt;/a&gt;中会用实际代码展示这些方法.&lt;/p&gt;

</content>
   </entry>
   

</feed>


</body>
</html>
